## 3.1 sklearn 转换器和预估器
### 3.1.1 转换器 - 特征工程的父类
1. 实例化 (实例化的是一个转换器类(Transformer))
2. 调用fit_transform
### 3.1.2 预估器(estimator)
1. 实例化一个estimator
2. estimator.fit(x_train, y_train) 计算
--调用完毕,模型生成
3. 模型评估:
    1) 直接比对真实值和预测值:
        y_predict = estimator.predict(x_test) 
        y_test == y_predict
    2) 计算准确率
        accuracy = estimator.score(x_test, y_test)

## 3.2 K-近邻算法(KNN)
### 3.2.1 核心思想:
K个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别(根据你的"邻居"来推断你的类型)
计算公式:欧式距离
### 3.2.2 KNN算法预估器:
    estimator = KNeighborsClassifier(n_neighbors)
    estimator.fit(x_train, y_train)
### 3.2.3 应用场景
使用数据量少的场景
### 3.2.4 KNN算法优缺点:
1. 优点: 简单,无需训练
2. 缺点: 懒惰算法,内存开销大;必须指定K值,精度无法保证

## 3.3 模型选择与调优
### 3.3.1 交叉验证(cross validation):
将拿到的数据分成N份,一份作验证集,N次测试,测试后取平均作结果,就叫N折交叉验证
训练集: 训练集+验证集
测试集: 测试集
### 3.3.2 超参数搜索(网络搜索 Grid Search)
超参数: 需要手动指定的参数
预设几种超参数组合,每组都采用交叉验证来评估


## 3.4 朴素贝叶斯算法(假设特征之间是独立的)
### 3.4.1 联合概率、条件概率、相互独立
联合概率P(A,B):包含多个条件,且所有条件同时成立的概率
条件概率P(A|B):A在B已经发生的条件下发生概率
相互独立:P(A,B) = P(A)P(B)

### 3.4.2 拉普拉斯平滑系数
防止分类计算分类概率为0

### 3.4.3 朴素贝叶斯算法预估器
    estimator = MultinomialNB()
    estimator.fit(x_train, y_train)

### 3.4.4 应用场景
文本分类;

### 3.4.5 朴素贝叶斯优缺点
1. 优点: 分类效率稳定,算法简单,速度快
2. 缺点: 如果特征有关联则效果不佳(数据上下文联系)

## 3.5 决策树
### 3.5.1 信息论基础
信息: 消除随机不定性的东西

### 3.5.2 决策依据: 信息增益
信息增益 = 信息熵 -条件熵
### 3.5.3 决策树预估器
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train, y_train)
### 3.5.4 决策树可视化
    保存树的结构到dot文件,再导入http://webgraphviz.com/显示
    export_graphviz(estimator, out_file="名.dot", feature_names=名.feature_names)
### 3.5.5 决策树优缺点
1. 优点: 简单,能可视化
2. 缺点: 容易过拟合

## 3.6 随机森林: 多个树组合输出总数
### 3.6.1 集成学习方法
建立多个模型组合来解决单一预测问题,多个分类器/模型,各自独立学习作出预测,最后合成组合预测

### 3.6.2 随机森林原理: 
1. 训练集随机: bootstrap,随机又放回抽样
2. 特征随机: 从M个特征中随机抽取m个特征,要求 M >> m

### 3.6.3 随机森林预估器
    estimator = RandomForestClassifier()
    # 参数准备
    param_dict = {"n_estimators": [],
                    "max_depth": [] }

### 3.6.4 应用场景
高纬度特征,大数据


### 3.6.5 优缺点
1. 优点: 准确率极高;能有效运行在大数据集上,有效处理高维特征且不需要降维;能评估各个特征在分类问题上的重要性
2. 缺点: 


